{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invisible-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clean-telling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBD\\v_bend_g01_c01.avi_frame0.jpg</td>\n",
       "      <td>bend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBD\\v_bend_g01_c01.avi_frame1.jpg</td>\n",
       "      <td>bend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBD\\v_bend_g01_c01.avi_frame2.jpg</td>\n",
       "      <td>bend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBD\\v_bend_g01_c01.avi_frame3.jpg</td>\n",
       "      <td>bend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBD\\v_bend_g02_c01.avi_frame0.jpg</td>\n",
       "      <td>bend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image class\n",
       "0  CBD\\v_bend_g01_c01.avi_frame0.jpg  bend\n",
       "1  CBD\\v_bend_g01_c01.avi_frame1.jpg  bend\n",
       "2  CBD\\v_bend_g01_c01.avi_frame2.jpg  bend\n",
       "3  CBD\\v_bend_g01_c01.avi_frame3.jpg  bend\n",
       "4  CBD\\v_bend_g02_c01.avi_frame0.jpg  bend"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('UCF/train_new.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attempted-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3951/3951 [00:51<00:00, 77.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3951, 224, 224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img(train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X = np.array(train_image)\n",
    "\n",
    "# shape of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "printable-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target\n",
    "y = train['class']\n",
    "\n",
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "environmental-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ready-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the base model of pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bigger-control",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3160, 7, 7, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train = base_model.predict(X_train)\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acceptable-silicon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791, 7, 7, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_test = base_model.predict(X_test)\n",
    "X_test = base_model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unique-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(3160, 7*7*512)\n",
    "X_test = X_test.reshape(791, 7*7*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worth-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization : Min=0.0, max=11.176948547363281\n",
      "After normalization  : Min=0.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "# normalizing the pixel values\n",
    "print('Before normalization : Min={}, max={}'.format(X_train.min(),X_train.max()))\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max\n",
    "print('After normalization  : Min={}, max={}'.format(X_train.min(),X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "behind-cuisine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3160, 25088)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of images\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "oriental-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu',input_shape=(25088,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "charitable-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "congressional-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faced-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 5s 480ms/step - loss: 0.0658 - accuracy: 0.9791 - val_loss: 0.0572 - val_accuracy: 0.9798\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 4s 375ms/step - loss: 0.0568 - accuracy: 0.9791 - val_loss: 0.0688 - val_accuracy: 0.9823\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 5s 415ms/step - loss: 0.0573 - accuracy: 0.9807 - val_loss: 0.0743 - val_accuracy: 0.9785\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 6s 528ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.0542 - val_accuracy: 0.9810\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 4s 391ms/step - loss: 0.0501 - accuracy: 0.9810 - val_loss: 0.0719 - val_accuracy: 0.9785\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 5s 460ms/step - loss: 0.0515 - accuracy: 0.9823 - val_loss: 0.0502 - val_accuracy: 0.9798\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 4s 394ms/step - loss: 0.0478 - accuracy: 0.9791 - val_loss: 0.0507 - val_accuracy: 0.9798\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 5s 466ms/step - loss: 0.0475 - accuracy: 0.9813 - val_loss: 0.0475 - val_accuracy: 0.9798\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 5s 410ms/step - loss: 0.0410 - accuracy: 0.9839 - val_loss: 0.0506 - val_accuracy: 0.9798\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 4s 362ms/step - loss: 0.0430 - accuracy: 0.9820 - val_loss: 0.0621 - val_accuracy: 0.9785\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 5s 467ms/step - loss: 0.0406 - accuracy: 0.9832 - val_loss: 0.0415 - val_accuracy: 0.9861\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 4s 368ms/step - loss: 0.0364 - accuracy: 0.9832 - val_loss: 0.0451 - val_accuracy: 0.9810\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 4s 374ms/step - loss: 0.0416 - accuracy: 0.9816 - val_loss: 0.0550 - val_accuracy: 0.9798\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 4s 374ms/step - loss: 0.0431 - accuracy: 0.9807 - val_loss: 0.0491 - val_accuracy: 0.9798\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 5s 470ms/step - loss: 0.0400 - accuracy: 0.9820 - val_loss: 0.0409 - val_accuracy: 0.9823\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 4s 361ms/step - loss: 0.0583 - accuracy: 0.9813 - val_loss: 0.0456 - val_accuracy: 0.9810\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 4s 374ms/step - loss: 0.0467 - accuracy: 0.9816 - val_loss: 0.0754 - val_accuracy: 0.9760\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 4s 372ms/step - loss: 0.0422 - accuracy: 0.9804 - val_loss: 0.0442 - val_accuracy: 0.9810\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 4s 402ms/step - loss: 0.0412 - accuracy: 0.9829 - val_loss: 0.0425 - val_accuracy: 0.9810\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 5s 497ms/step - loss: 0.0397 - accuracy: 0.9823 - val_loss: 0.0379 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26ef6db9f40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-princeton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
